{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import time\n", "import os"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Test if we are running in a notebook<br>\n", "https://stackoverflow.com/questions/15411967/how-can-i-check-if-code-is-executed-in-the-ipython-notebook"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["in_notebook = False\n", "try:\n", "    # get_ipython() is only defined inside ipython; we have to ignore it in flake8 with noqa\n", "    get_ipython()  # noqa F821\n", "    in_notebook = True\n", "except(NameError):\n", "    in_notebook = False "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Make the cells wider than the default:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if in_notebook:\n", "    display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n", "    if not os.path.exists(\"flatland-installed.flag\"):\n", "        print(\"Running install steps for Google Colab - assuming apt works!\")\n", "        os.system(\"apt-get install -y xvfb python-opengl\")\n", "        os.system(\"apt install x11-utils\")\n", "        os.system(\"pip install pyvirtualdisplay\")\n", "        os.system(\"pip install flatland-rl\")\n", "        os.system(\"touch ./flatland-installed.flag\")\n", "        print(\"You may need to restart runtime on Colab now...\")\n", "    else:\n", "        print(\"Looks like flatland-rl and reqts are already installed - skipping install\")\n", "    print(\"Starting Xorg xvfb virtual display for Colab\")\n", "    import pyvirtualdisplay\n", "    xdisplay = pyvirtualdisplay.Display(visible=0, size=(400, 300))\n", "    print(xdisplay.start())"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np  # noqa e402"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In Flatland you can use custom observation builders and predicitors<br>\n", "Observation builders generate the observation needed by the controller<br>\n", "Preditctors can be used to do short time prediction which can help in avoiding conflicts in the network"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from flatland.envs.malfunction_generators import malfunction_from_params, MalfunctionParameters\n", "from flatland.envs.observations import GlobalObsForRailEnv\n", "# First of all we import the Flatland rail environment\n", "from flatland.envs.rail_env import RailEnv\n", "from flatland.envs.rail_env import RailEnvActions\n", "from flatland.envs.rail_generators import sparse_rail_generator\n", "from flatland.envs.schedule_generators import sparse_schedule_generator\n", "# We also include a renderer because we want to visualize what is going on in the environment\n", "from flatland.utils.rendertools import RenderTool, AgentRenderVariant"]}, {"cell_type": "markdown", "metadata": {}, "source": ["These are used in the notebook version of this code, but not the plain python"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from IPython.core.display import display, HTML, clear_output\n", "import PIL"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This is an introduction example for the Flatland 2.1.* version.<br>\n", "Changes and highlights of this version include<br>\n", "- Stochastic events (malfunctions)<br>\n", "- Different travel speeds for differet agents<br>\n", "- Levels are generated using a novel generator to reflect more realistic railway networks<br>\n", "- Agents start outside of the environment and enter at their own time<br>\n", "- Agents leave the environment after they have reached their goal<br>\n", "Use the new sparse_rail_generator to generate feasible network configurations with corresponding tasks<br>\n", "Training on simple small tasks is the best way to get familiar with the environment<br>\n", "We start by importing the necessary rail and schedule generators<br>\n", "The rail generator will generate the railway infrastructure<br>\n", "The schedule generator will assign tasks to all the agent within the railway network"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The railway infrastructure can be build using any of the provided generators in env/rail_generators.py<br>\n", "Here we use the sparse_rail_generator with the following parameters"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if in_notebook:  # use a smaller map in the notebook\n", "    width = 30  # With of map\n", "    height = 30  # Height of map\n", "    nr_trains = 5  # Number of trains that have an assigned task in the env\n", "    cities_in_map = 2  # Number of cities where agents can start or end\n", "else:\n", "    width = 16 * 7  # With of map\n", "    height = 9 * 7  # Height of map\n", "    nr_trains = 50  # Number of trains that have an assigned task in the env\n", "    cities_in_map = 20  # Number of cities where agents can start or end\n", "    \n", "seed = 14  # Random seed\n", "grid_distribution_of_cities = False  # Type of city distribution, if False cities are randomly placed\n", "max_rails_between_cities = 2  # Max number of tracks allowed between cities. This is number of entry point to a city\n", "max_rail_in_cities = 6  # Max number of parallel tracks within a city, representing a realistic trainstation"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["rail_generator = sparse_rail_generator(max_num_cities=cities_in_map,\n", "                                       seed=seed,\n", "                                       grid_mode=grid_distribution_of_cities,\n", "                                       max_rails_between_cities=max_rails_between_cities,\n", "                                       max_rails_in_city=max_rail_in_cities,\n", "                                       )"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The schedule generator can make very basic schedules with a start point, end point and a speed profile for each agent.<br>\n", "The speed profiles can be adjusted directly as well as shown later on. We start by introducing a statistical<br>\n", "distribution of speed profiles"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Different agent types (trains) with different speeds."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["speed_ration_map = {1.: 0.25,  # Fast passenger train\n", "                    1. / 2.: 0.25,  # Fast freight train\n", "                    1. / 3.: 0.25,  # Slow commuter train\n", "                    1. / 4.: 0.25}  # Slow freight train"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can now initiate the schedule generator with the given speed profiles"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["schedule_generator = sparse_schedule_generator(speed_ration_map)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can furthermore pass stochastic data to the RailEnv constructor which will allow for stochastic malfunctions<br>\n", "during an episode."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["stochastic_data = MalfunctionParameters(malfunction_rate=10000,  # Rate of malfunction occurence\n", "                                        min_duration=15,  # Minimal duration of malfunction\n", "                                        max_duration=50  # Max duration of malfunction\n", "                                        )\n", "# Custom observation builder without predictor\n", "observation_builder = GlobalObsForRailEnv()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Custom observation builder with predictor, uncomment line below if you want to try this one<br>\n", "observation_builder = TreeObsForRailEnv(max_depth=2, predictor=ShortestPathPredictorForRailEnv())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Construct the enviornment with the given observation, generataors, predictors, and stochastic data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["env = RailEnv(width=width,\n", "              height=height,\n", "              rail_generator=rail_generator,\n", "              schedule_generator=schedule_generator,\n", "              number_of_agents=nr_trains,\n", "              obs_builder_object=observation_builder,\n", "              malfunction_generator_and_process_data=malfunction_from_params(stochastic_data),\n", "              remove_agents_at_target=True)\n", "env.reset()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Initiate the renderer"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["env_renderer = RenderTool(env, gl=\"PILSVG\",\n", "                          agent_render_variant=AgentRenderVariant.ONE_STEP_BEHIND,\n", "                          show_debug=False,\n", "                          screen_height=600,  # Adjust these parameters to fit your resolution\n", "                          screen_width=800)  # Adjust these parameters to fit your resolution"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The first thing we notice is that some agents don't have feasible paths to their target.<br>\n", "We first look at the map we have created"]}, {"cell_type": "markdown", "metadata": {}, "source": ["nv_renderer.render_env(show=True)<br>\n", "time.sleep(2)<br>\n", "Import your own Agent or use RLlib to train agents on Flatland<br>\n", "As an example we use a random agent instead"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class RandomAgent:\n", "    def __init__(self, state_size, action_size):\n", "        self.state_size = state_size\n", "        self.action_size = action_size\n", "    def act(self, state):\n", "        \"\"\"\n", "        :param state: input is the observation of the agent\n", "        :return: returns an action\n", "        \"\"\"\n", "        return np.random.choice([RailEnvActions.MOVE_FORWARD, RailEnvActions.MOVE_RIGHT, RailEnvActions.MOVE_LEFT,\n", "                                 RailEnvActions.STOP_MOVING])\n", "    def step(self, memories):\n", "        \"\"\"\n", "        Step function to improve agent by adjusting policy given the observations\n", "        :param memories: SARS Tuple to be\n", "        :return:\n", "        \"\"\"\n", "        return\n", "    def save(self, filename):\n", "        # Store the current policy\n", "        return\n", "    def load(self, filename):\n", "        # Load a policy\n", "        return"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Initialize the agent with the parameters corresponding to the environment and observation_builder"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["controller = RandomAgent(218, env.action_space[0])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We start by looking at the information of each agent<br>\n", "We can see the task assigned to the agent by looking at"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"\\n Agents in the environment have to solve the following tasks: \\n\")\n", "for agent_idx, agent in enumerate(env.agents):\n", "    print(\n", "        \"The agent with index {} has the task to go from its initial position {},\" +\n", "        \"facing in the direction {} to its target at {}.\".format(\n", "            agent_idx, agent.initial_position, agent.direction, agent.target))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The agent will always have a status indicating if it is currently present in the environment or done or active<br>\n", "For example we see that agent with index 0 is currently not active"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"\\n Their current statuses are:\")\n", "print(\"============================\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for agent_idx, agent in enumerate(env.agents):\n", "    print(\"Agent {} status is: {} with its current position being {}\".format(agent_idx, str(agent.status),\n", "                                                                             str(agent.position)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The agent needs to take any action [1,2,3] except do_nothing or stop to enter the level<br>\n", "If the starting cell is free they will enter the level<br>\n", "If multiple agents want to enter the same cell at the same time the lower index agent will enter first."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's check if there are any agents with the same start location"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["agents_with_same_start = set()\n", "print(\"\\n The following agents have the same initial position:\")\n", "print(\"=====================================================\")\n", "for agent_idx, agent in enumerate(env.agents):\n", "    for agent_2_idx, agent2 in enumerate(env.agents):\n", "        if agent_idx != agent_2_idx and agent.initial_position == agent2.initial_position:\n", "            print(\"Agent {} as the same initial position as agent {}\".format(agent_idx, agent_2_idx))\n", "            agents_with_same_start.add(agent_idx)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Lets try to enter with all of these agents at the same time"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["action_dict = dict()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for agent_id in agents_with_same_start:\n", "    action_dict[agent_id] = 1  # Try to move with the agents"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Do a step in the environment to see what agents entered:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["env.step(action_dict)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Current state and position of the agents after all agents with same start position tried to move"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"\\n This happened when all tried to enter at the same time:\")\n", "print(\"========================================================\")\n", "for agent_id in agents_with_same_start:\n", "    print(\n", "        \"Agent {} status is: {} with the current position being {}.\".format(\n", "            agent_id, str(env.agents[agent_id].status),\n", "            str(env.agents[agent_id].position)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As you see only the agents with lower indexes moved. As soon as the cell is free again the agents can attempt<br>\n", "to start again."]}, {"cell_type": "markdown", "metadata": {}, "source": ["You will also notice, that the agents move at different speeds once they are on the rail.<br>\n", "The agents will always move at full speed when moving, never a speed inbetween.<br>\n", "The fastest an agent can go is 1, meaning that it moves to the next cell at every time step<br>\n", "All slower speeds indicate the fraction of a cell that is moved at each time step<br>\n", "Lets look at the current speed data of the agents:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"\\n The speed information of the agents are:\")\n", "print(\"=========================================\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for agent_idx, agent in enumerate(env.agents):\n", "    print(\n", "        \"Agent {} speed is: {:.2f} with the current fractional position being {}\".format(\n", "            agent_idx, agent.speed_data['speed'], agent.speed_data['position_fraction']))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["New the agents can also have stochastic malfunctions happening which will lead to them being unable to move<br>\n", "for a certain amount of time steps. The malfunction data of the agents can easily be accessed as follows"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"\\n The malfunction data of the agents are:\")\n", "print(\"========================================\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for agent_idx, agent in enumerate(env.agents):\n", "    print(\n", "        \"Agent {} is OK = {}\".format(\n", "            agent_idx, agent.malfunction_data['malfunction'] < 1))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now that you have seen these novel concepts that were introduced you will realize that agents don't need to take<br>\n", "an action at every time step as it will only change the outcome when actions are chosen at cell entry.<br>\n", "Therefore the environment provides information about what agents need to provide an action in the next step.<br>\n", "You can access this in the following way."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Chose an action for each agent"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for a in range(env.get_num_agents()):\n", "    action = controller.act(0)\n", "    action_dict.update({a: action})\n", "# Do the environment step\n", "observations, rewards, dones, information = env.step(action_dict)\n", "print(\"\\n The following agents can register an action:\")\n", "print(\"========================================\")\n", "for info in information['action_required']:\n", "    print(\"Agent {} needs to submit an action.\".format(info))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We recommend that you monitor the malfunction data and the action required in order to optimize your training<br>\n", "and controlling code."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let us now look at an episode playing out with random actions performed"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"\\nStart episode...\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Reset the rendering system"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["env_renderer.reset()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Here you can also further enhance the provided observation by means of normalization<br>\n", "See training navigation example in the baseline repository"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["score = 0\n", "# Run episode\n", "frame_step = 0"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if in_notebook:\n", "    nSteps = 5  # just 5 steps at a time - rerun the cell for more steps\n", "else:\n", "    nSteps = 500"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for step in range(nSteps):\n", "    # Chose an action for each agent in the environment\n", "    for a in range(env.get_num_agents()):\n", "        action = controller.act(observations[a])\n", "        action_dict.update({a: action})\n\n", "    # Environment step which returns the observations for all agents, their corresponding\n", "    # reward and whether their are done\n", "    next_obs, all_rewards, done, _ = env.step(action_dict)\n", "    env_renderer.render_env(show=not in_notebook, show_observations=False, show_predictions=False)\n", "    # env_renderer.gl.save_image('./misc/Fames2/flatland_frame_{:04d}.png'.format(step))\n", "    frame_step += 1\n", "    # Update replay buffer and train agent\n", "    for a in range(env.get_num_agents()):\n", "        controller.step((observations[a], action_dict[a], all_rewards[a], next_obs[a], done[a]))\n", "        score += all_rewards[a]\n", "    observations = next_obs.copy()\n", "    if done['__all__']:\n", "        break\n", "    print('Episode: Steps {}\\t Score = {}'.format(step, score))\n", "    if in_notebook:\n", "        arrImage = env_renderer.get_image()\n", "        pilImage = PIL.Image.fromarray(arrImage)\n", "        clear_output()\n", "        display(pilImage)\n", "        time.sleep(0.3)\n", "    "]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}